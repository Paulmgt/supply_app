import pandas as pd
import re
import os
import joblib
from transformers import MarianMTModel, MarianTokenizer
from langdetect import detect
from concurrent.futures import ThreadPoolExecutor, as_completed
import psutil
import torch


def f_data_clean_2():
    # Taille des lots pour la traduction
    BATCH_SIZE = 50  # RÃ©duire la taille des lots pour mieux exploiter la parallÃ©lisation

    # Calcul dynamique du nombre de threads en fonction des cÅ“urs CPU
    max_workers = min(32, os.cpu_count() + 4)  # Ex: limite Ã  32 threads maximum, ou CPU + 4
    
    # RÃ©cupÃ©ration des informations sur la RAM
    available_memory = psutil.virtual_memory().available / (1024 ** 3)  # RAM disponible en Go

    # Fichier log
    fichier_c = open("src/features/log_clean_c.txt", "a")
    print("------------- dÃ©but clean data: features engineering -----------------", file=fichier_c)
    print(f"RAM disponible: {available_memory:.2f} Go", file=fichier_c)

    # VÃ©rification de la prÃ©sence du fichier brut
    fichier_brut = 'src/data/Final_data_scraped_traitÃ©_non_traduit_latest.csv'

    if not os.path.isfile(fichier_brut):
        print(f"Le fichier {fichier_brut} n'existe pas. Veuillez vÃ©rifier le chemin du fichier.", file=fichier_c)
        return []  # Retourner une liste vide si le fichier n'existe pas

    # Chargement du fichier brut non traitÃ©
    df = pd.read_csv(fichier_brut)

    # Ajout de nouvelles colonnes avec des caractÃ©ristiques de texte
    df['nombre_caractÃ¨res'] = df['commentaire'].apply(len)
    df['nombre_maj'] = df['commentaire'].apply(lambda x: sum(1 for c in x if c.isupper()))
    df['nombre_car_spÃ©'] = df['commentaire'].apply(lambda x: len([c for c in x if not c.isalnum() and not c.isspace()]))
    df['caractÃ¨res_spÃ©'] = df['commentaire'].apply(lambda x: [c for c in x if not c.isalnum() and not c.isspace() and c not in ['.', ',', '?', '!']])

    # Liste d'emojis positifs et nÃ©gatifs
    emojis_positifs = ['ğŸ˜€', 'ğŸ˜', 'ğŸ˜‚', 'ğŸ¤£', 'ğŸ˜ƒ', 'ğŸ˜„', 'ğŸ˜…', 'ğŸ˜†', 'ğŸ˜‡', 'ğŸ˜‰', 'ğŸ˜Š', 'ğŸ˜‹', 'ğŸ˜Œ', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜‘','ğŸ‘', 'ğŸ‘', 'ğŸ™Œ', 'ğŸ¤', 'ğŸ™', 'âœŒï¸','âœŒ', 'ğŸ¤', 'ğŸ¤Ÿ', 'ğŸ¤˜', 'ğŸ¤™', 'ğŸ‘Œ', 'ğŸ‘ˆ', 'ğŸ‘‰', 'ğŸ‘†', 'ğŸ‘‡', 'â˜ï¸', 'âœ‹', 'ğŸ¤š', 'ğŸ–ï¸', 'ğŸ––','ğŸ‘‹', 'ğŸ¤—', 'ğŸ¤©','ğŸ’–','ğŸ’“', 'ğŸ’•', 'ğŸ’', 'ğŸ’˜' ,'ğŸ’—' ,'ğŸ’','â¤ï¸','ğŸ§¡' ,'ğŸ’›' ,'ğŸ’š', 'ğŸ’™' ,'ğŸ’œ' ,'ğŸ¤', 'ğŸ–¤','â¤' ,'ğŸ¤', 'ğŸ’Ÿ', 'ğŸ’«', 'ğŸ’¯']
    emojis_negatifs = ['ğŸ˜”', 'ğŸ˜•', 'ğŸ˜–', 'ğŸ˜£', 'ğŸ˜¢', 'ğŸ˜¥', 'ğŸ˜°', 'ğŸ˜¨', 'ğŸ˜©', 'ğŸ˜«', 'ğŸ˜¤', 'ğŸ˜¡', 'ğŸ˜ ', 'ğŸ˜ˆ', 'ğŸ‘¿', 'ğŸ’€', 'â˜ ï¸', 'ğŸ’©', 'ğŸ¤¡','ğŸ‘','ğŸ‘Š', 'ğŸ–•','ğŸ’”']

    # Fonction pour compter les emojis
    def compter_emojis(texte, emojis_list):
        return sum(1 for char in texte if char in emojis_list)

    df['emojis_positifs_count'] = df['caractÃ¨res_spÃ©'].apply(lambda x: compter_emojis(''.join(x), emojis_positifs))
    df['emojis_negatifs_count'] = df['caractÃ¨res_spÃ©'].apply(lambda x: compter_emojis(''.join(x), emojis_negatifs))

    # Nettoyage des caractÃ¨res spÃ©ciaux
    def remove_special_characters_and_emojis(text):
        text = re.sub(r'[^a-zA-Z0-9\sÃ Ã¡Ã¢Ã¤Ã§Ã¨Ã©ÃªÃ«Ã¬Ã­Ã®Ã¯Ã±Ã²Ã³Ã´Ã¶Ã¹ÃºÃ»Ã¼Ã½Ã¿\s,.;\']', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    df['commentaire_text'] = df['commentaire'].apply(remove_special_characters_and_emojis)

    # Conversion en minuscules
    df['commentaire_text'] = df['commentaire_text'].str.lower()

    # DÃ©tection de la langue
    def detect_language_safe(text):
        try:
            return detect(text)
        except:
            return 'Non dÃ©tectÃ©e'

    df['langue_bis'] = df['commentaire_text'].apply(lambda text: detect_language_safe(text) if text else 'Non dÃ©tectÃ©e')

    # Suppression des lignes avec langue "Non dÃ©tectÃ©e"
    df = df[df['langue_bis'] != 'Non dÃ©tectÃ©e']

    # Chargement de MarianMT pour la traduction
    model_name = 'Helsinki-NLP/opus-mt-mul-en'  # Traduction multilingue vers l'anglais
    tokenizer = MarianTokenizer.from_pretrained(model_name)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = MarianMTModel.from_pretrained(model_name).to(device)

    def traduire_lot(batch):
        for index, row in batch.iterrows():
            try:
                if row['langue_bis'] != 'en':
                    inputs = tokenizer(row['commentaire_text'], return_tensors="pt", padding=True, truncation=True).to(device)
                    translated = model.generate(**inputs)
                    batch.at[index, 'commentaire_text'] = tokenizer.decode(translated[0], skip_special_tokens=True)
                # Pas besoin d'autre chose si `commentaire_text` est dÃ©jÃ  en anglais
            except Exception as e:
                print(f"Erreur de traduction pour l'index {index}: {e}", file=fichier_c)
                # Pas besoin de mettre Ã  jour `commentaire_text` si une erreur survient, il reste inchangÃ©
        return batch


    # Traitement par lots pour la traduction
    df_batches = [df[i:i + BATCH_SIZE] for i in range(0, len(df), BATCH_SIZE)]

    translated_batches = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:  # Utilisation de max_workers dynamiques
        futures = {executor.submit(traduire_lot, batch): batch for batch in df_batches}
        for future in as_completed(futures):
            translated_batches.append(future.result())

    # Combinaison de tous les lots traduits
    df_translated = pd.concat(translated_batches)

    # GÃ©rer les valeurs NaN avant la conversion
    def handle_nan_values(df):
        # Remplacer NaN ou chaÃ®nes vides par des valeurs par dÃ©faut appropriÃ©es
        df['notes'] = pd.to_numeric(df['notes'], errors='coerce').fillna(0)  # Convertir en float, remplacer NaN par 0
        df['annÃ©e_experience'] = pd.to_numeric(df['annÃ©e_experience'], errors='coerce').fillna(0).astype(int)  # Convertir en int, remplacer NaN par 0
        df['mois_experience'] = pd.to_numeric(df['mois_experience'], errors='coerce').fillna(0).astype(int)
        df['jour_experience'] = pd.to_numeric(df['jour_experience'], errors='coerce').fillna(0).astype(int)
        df['annÃ©e_commentaire'] = pd.to_numeric(df['annÃ©e_commentaire'], errors='coerce').fillna(0).astype(int)
        df['mois_commentaire'] = pd.to_numeric(df['mois_commentaire'], errors='coerce').fillna(0).astype(int)
        df['jour_commentaire'] = pd.to_numeric(df['jour_commentaire'], errors='coerce').fillna(0).astype(int)
        
        # Convertir la liste de 'caractÃ¨res_spÃ©' en chaÃ®ne de caractÃ¨res
        df['caractÃ¨res_spÃ©'] = df['caractÃ¨res_spÃ©'].apply(lambda x: ''.join(x) if isinstance(x, list) else x)

        # Remplacer les valeurs vides dans 'leadtime_com_exp' par 0 (ou une autre valeur par dÃ©faut)
        df['leadtime_com_exp'] = pd.to_numeric(df['leadtime_com_exp'], errors='coerce').fillna(0)
        
        # Remplacer NaN par une chaÃ®ne vide pour les colonnes de type string
        df = df.fillna('')

        # Remplacer NaN par 0 pour les colonnes de type nombre
        numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns
        df[numeric_cols] = df[numeric_cols].fillna(0)

        return df

    df_cleaned = handle_nan_values(df_translated)

    new_data_lib = f'src/models/new_data_lib'
    joblib.dump(df_cleaned, new_data_lib)

    print(f"fichier nettoyÃ©: {new_data_lib}", file=fichier_c)
    print("------------- fin clean data -----------------", file=fichier_c)
    fichier_c.close()

    return df_cleaned  # Retourner les donnÃ©es nettoyÃ©es


if __name__ == "__main__":
    f_data_clean_2()